<details>
<summary>가상화에 대해 설명해주세요</summary>
<div>

##### 가상화의 개념
- 가상화는 물리적 컴퓨터 하드웨어를 추상화하여 논리적인 리소스를 만들어내는 기술입니다.
- 이를 통해 단일 물리 머신을 마치 여러 대의 머신처럼 사용할 수 있으며, 리소스를 효율적으로 활용할 수 있습니다.
- 가상화는 1960년대부터 시작되었지만, 2000년대 초반 하이퍼바이저 기술이 발전하면서 널리 보급되었습니다. (현재는 컨테이너 가상화가 대세)
##### 가상화의 장점
- 하드웨어 자원의 효율적 활용: 단일 물리 머신을 여러 개의 가상 머신으로 분할하여 사용할 수 있습니다.
- 비용 절감: 하드웨어 투자 비용을 줄일 수 있습니다.
- 유연성 및 확장성 향상: 필요에 따라 가상 머신을 쉽게 생성, 삭제, 이동할 수 있습니다.
- 가용성 및 재해 복구 향상: 가상 머신을 백업하여 장애 발생 시 신속하게 복구할 수 있습니다.
##### 가상화의 활용 분야
- 서버 가상화: 단일 물리 서버에 여러 개의 가상 서버를 구축하여 운영할 수 있습니다.
- 데스크톱 가상화: 사용자 PC의 운영체제, 애플리케이션, 데이터를 가상화하여 중앙에서 관리할 수 있습니다.
- 네트워크 가상화: 물리적 네트워크 장비를 가상화하여 유연한 네트워크 구성이 가능합니다.
- 스토리지 가상화: 물리적 스토리지 장비를 가상화하여 논리적 스토리지 풀을 구성할 수 있습니다.

</div>
</details>

<details>
<summary>
네트워크 가상화에 대해
</summary>
<div>

네트워크 가상화는 네트워크 자원을 추상화하여 물리 네트워크 인프라 위에서 다양한 가상 네트워크를 동시에 운영할 수 있도록 만드는 기술입니다. 이를 통해 하드웨어의 물리적 한계를 넘어서서 네트워크 자원을 더 유연하고 효율적으로 사용할 수 있습니다. 
#### 구현 방법
- VLAN (Virtual Local Area Network): VLAN은 물리적 위치에 상관없이 특정 네트워크 기기들을 같은 네트워크로 묶을 수 있게 해주는 기술입니다. 이를 통해 여러 개의 가상 LAN을 하나의 물리적 네트워크 상에서 운영할 수 있습니다.

- VPN (Virtual Private Network): VPN은 공용 인터넷을 통해 가상의 개인 네트워크를 구축하는 기술입니다. 데이터를 암호화하여 안전하게 전송할 수 있으며, 원격지에서도 내부 네트워크에 안전하게 접속할 수 있게 해줍니다.

- SDN (Software Defined Networking): SDN은 네트워크 제어를 프로그래밍 가능한 중앙 집중식 소프트웨어로 이동시켜, 네트워크 하드웨어의 설정과 관리를 소프트웨어를 통해 자동화합니다. 이를 통해 네트워크 자원을 더 유연하고 효율적으로 관리할 수 있습니다.

- NFV (Network Function Virtualization): NFV는 네트워크 기능을 가상화하여 전통적인 네트워크 하드웨어 장비 없이도 네트워크 서비스를 제공할 수 있게 하는 기술입니다. 예를 들어, 가상 방화벽, 가상 로드 밸런서 등을 소프트웨어로 구현할 수 있습니다.

네트워크 가상화를 통해 네트워크 설계와 운영의 복잡성을 줄이고, 비용을 절감하며, 네트워크 자원의 활용도를 높일 수 있습니다. 또한, 네트워크 서비스와 애플리케이션의 배포 속도를 높여 비즈니스의 민첩성을 향상시킬 수 있습니다.
</div>
</details>

<details>
<summary>가상 머신과 컨테이너에 대해 설명해주세요</summary>
<div>

#### 가상 머신(VM)
가가상 머신은 하이퍼바이저(Hypervisor)라는 소프트웨어를 통해 물리적 하드웨어를 추상화하여 만든 가상의 컴퓨터 환경입니다. 각 가상 머신은 완전한 운영 체제를 포함하며, 하드웨어 자원(CPU, 메모리 등)을 할당 받아 독립적으로 실행됩니다. 이는 서로 다른 운영 체제를 단일 물리 서버에서 동시에 실행할 수 있게 해줍니다. VM은 보안과 격리 수준이 높지만, 운영 체제를 포함한 전체 가상화로 인해 상대적으로 더 많은 리소스를 소모하고 성능 오버헤드가 있을 수 있습니다.(무거운 편이면 시작 및 실행 속도가 느림)

#### 컨테이너
컨테이너는 애플리케이션과 그 종속성을 함께 패키징하여, 어느 환경에서든 동일하게 실행될 수 있도록 설계된 가상화 기술입니다. 컨테이너는 운영 체제 수준에서 가상화되며, 단일 호스트 운영 체제의 커널을 여러 컨테이너가 공유합니다. 따라서 각 컨테이너는 가벼우며, 빠르게 시작되고 종료됩니다. 컨테이너는 애플리케이션 개발과 배포를 더욱 빠르고 효율적으로 만들어주지만, VM에 비해 상대적으로 낮은 격리 수준을 제공합니다.

##### 비교
- 격리 수준: VM은 하드웨어 수준에서 완전히 격리된 환경을 제공하는 반면, 컨테이너는 운영 체제 수준에서 가벼운 격리를 제공합니다.
- 성능: 컨테이너는 운영 체제를 공유하기 때문에 VM보다 리소스 소모가 적고, 시작 시간이 빠릅니다.
- 용도: VM은 높은 격리와 보안이 필요한 환경에 적합하고, 컨테이너는 개발과 배포의 속도와 효율성을 중시하는 환경에 적합합니다.
</div>
</details>

<details>
<summary>가상 머신의 장단점</summary>
<div>

#### 장점
- 격리성(Isolation): 각 가상 머신은 독립된 실행 환경을 가지므로, 한 VM에서 발생하는 문제가 다른 VM에 영향을 주지 않습니다. 이로 인해 보안과 안정성이 향상됩니다.
- 유연성(Flexibility): 다양한 운영 체제를 단일 물리 서버에서 실행할 수 있어, 자원을 효율적으로 활용하고 필요에 따라 쉽게 환경을 변경할 수 있습니다.
- 이식성(Portability): 가상 머신은 컴퓨터의 하드웨어 환경에 구애받지 않고, 다른 물리적 또는 가상의 시스템으로 쉽게 이동할 수 있습니다.
- 복구 용이성: 가상 머신의 스냅샷을 생성하여 시스템의 특정 시점을 저장할 수 있으며, 문제 발생 시 빠르게 이전 상태로 복구할 수 있습니다.
#### 단점
- 성능 오버헤드(Performance Overhead): 가상 머신은 완전한 운영 체제를 실행하기 때문에, 물리적 시스템에 비해 추가적인 리소스(메모리, CPU 등)를 소모하며, 이로 인해 성능 저하가 발생할 수 있습니다.
- 초기 구축 비용 증가: 가상화 소프트웨어와 하드웨어 구매 비용이 추가로 발생합니다
- 관리 복잡성: 여러 가상 머신을 관리하는 것은 복잡할 수 있으며, 효율적인 리소스 관리와 보안 정책 유지에 주의가 필요합니다.

</div>
</details>

<details>
<summary>컨테이너의 장단점에 대해 설명해주세요</summary>
<div>

#### 장점
- 경량화: 컨테이너는 운영 체제(OS)의 커널을 여러 컨테이너가 공유하기 때문에, VM보다 훨씬 적은 리소스를 사용합니다. 이로 인해 빠른 시작과 더 효율적인 리소스 사용이 가능합니다.
- 이식성: 컨테이너는 어플리케이션과 그 의존성을 함께 패키징하기 때문에, 개발 환경에서 운영 환경까지 어떤 환경에서도 동일하게 실행될 수 있습니다.
- 빠른 배포와 확장성: 컨테이너의 가벼움은 빠른 시작을 가능하게 하며, 이는 배포 속도를 향상시키고 확장성을 높입니다.
- 지속적 통합 및 지속적 배포(CI/CD) 지원: 컨테이너는 CI/CD 파이프라인과 잘 통합되어, 빠른 반복 개발과 테스트, 배포를 지원합니다.
#### 단점
- 보안 취약성: 컨테이너는 가상 머신에 비해 격리 수준이 낮기 때문에, 하나의 컨테이너에서 발생한 보안 문제가 호스트 OS나 다른 컨테이너에 영향을 줄 수 있습니다.
- 관리 복잡성: 대규모 컨테이너 환경은 관리가 복잡해질 수 있으며, 이를 위해 Kubernetes와 같은 오케스트레이션 도구가 필요할 수 있습니다.
- 스토리지와 네트워킹: 컨테이너는 일시적인 성격을 가지고 있어, 영속적인 데이터 저장이나 복잡한 네트워킹 설정에 있어서 추가적인 고려가 필요합니다.

</div>
</details>

<details>
<summary>Docker란?</summary>
<div>

docker란 애플리케이션을 컨테이너화 하여 개발, 배포, 실행을 용이하게 해주는 오픈 소스 컨테이너화 플랫폼입니다. 컨테이너는 코드, 런타임, 시스템 도구, 시스템 라이브러리 등 애플리케이션이 실행되는데 필요한 모든 것을 포함하며, 이를 통해 애플리케이션이 다양한 환경에서 일관되게 작동할 수 있도록 합니다.

### 도커의 장점

- 컨테이너화: 애플리케이션과 그 종속성을 컨테이너 내에 패키징하여, 소프트웨어를 깨끗하고 일관된 환경에서 실행할 수 있도록 합니다.

- 이식성: 컨테이너는 Docker가 설치된 모든 시스템에서 실행될 수 있으므로, 개발자는 코드가 다른 환경에서 "그냥 작동한다"는 것을 확신할 수 있습니다.

- 버전 관리 및 재현성: Docker 이미지는 애플리케이션의 특정 상태를 캡처하며, 이를 버전 관리하고 필요할 때 쉽게 롤백할 수 있습니다.

- 개발 및 배포의 일관성: 개발부터 테스팅, 프로덕션에 이르기까지 동일한 Docker 컨테이너를 사용함으로써, 소프트웨어 배포의 일관성과 효율성을 높일 수 있습니다.

- 마이크로서비스 아키텍처: Docker는 마이크로서비스 아키텍처를 채택하는 데 이상적인 플랫폼을 제공합니다. 각 서비스를 독립적인 컨테이너로 배포함으로써, 복잡한 애플리케이션을 보다 쉽게 관리하고 확장할 수 있습니다.

### 도커의 단점

- 일부 애플리케이션 호환성 문제: 도커는 리눅스 운영체제에서 실행 가능한 소프트웨어만 지원하므로, 윈도우나 macOS 환경에서 실행되는 일부 애플리케이션은 호환성 문제가 있을 수 있습니다.
- 보안 이슈: 도커 컨테이너는 호스트 운영체제와 격리되어 있지만, 여전히 보안 취약점이 존재할 수 있습니다. 이를 해결하기 위해 보안 모범 사례를 준수해야 합니다.
- 복잡한 네트워킹: 도커 컨테이너 간 네트워킹을 설정하고 관리하는 것이 복잡할 수 있습니다. 이를 해결하기 위해 오케스트레이션 도구(Kubernetes 등)가 필요합니다.
- 데이터 영속성 문제: 도커 컨테이너는 일회성 실행 환경이므로, 컨테이너 내부의 데이터는 영속적이지 않습니다. 이를 해결하기 위해 볼륨 마운팅 등의 기술이 필요합니다. 

</div>
</details>

<details>
<summary>도커와 VM의 차이에 대해서 설명해보라</summary>
<div>


1. 가상화 방식
   - VM은 하이퍼바이저를 사용하여 물리적 서버를 여러 개의 가상 서버로 분할하고, 각각의 가상 서버에 독립적인 운영체제를 설치합니다.
   - 도커는 컨테이너 기술을 사용하여 애플리케이션과 그 실행에 필요한 라이브러리, 종속성 등을 패키징합니다. 컨테이너는 호스트 운영체제의 커널을 공유하므로 별도의 운영체제가 필요하지 않습니다.
2. 리소스 사용량
   - VM은 각각의 가상 서버에 운영체제가 필요하므로 리소스 사용량이 많습니다.
   - 도커 컨테이너는 호스트 운영체제의 커널을 공유하므로 리소스 사용량이 상대적으로 적습니다.
3. 배포 및 이식성
   - VM은 전체 운영체제를 포함하므로 배포와 이식성이 상대적으로 복잡합니다.
   - 도커 컨테이너는 애플리케이션과 종속성만 포함하므로 배포와 이식성이 용이합니다.
4. 보안
   - VM은 각각의 가상 서버가 독립적이므로 보안이 상대적으로 강합니다.
   - 도커 컨테이너는 호스트 운영체제를 공유하므로 보안 이슈가 발생할 수 있습니다. 이를 해결하기 위해 보안 모범 사례를 준수해야 합니다. 

</div>
</details>

<details>
<summary>Kubernetes란?</summary>
<div>

#### 쿠버네티스 개요
Kubernetes는 컨테이너화된 애플리케이션의 배포, 확장 및 관리를 자동화하기 위한 오픈 소스 시스템입니다. Google이 처음 개발했으며, 현재는 Cloud Native Computing Foundation(CNCF)에서 관리하고 있습니다. Kubernetes는 컨테이너의 배치와 스케일링을 간소화하고, 애플리케이션의 가용성을 높이며, 개발과 운영의 일관성을 유지하는 등 다양한 이점을 제공합니다.
#### 쿠버네티스의 주요 기능
- 컨테이너 배포 및 관리: 컨테이너화된 애플리케이션의 배포, 스케일링, 업데이트 등을 자동화합니다.
- 고가용성: 복제본 관리, 자동 복구 등을 통해 애플리케이션의 가용성을 높입니다.
- 서비스 디스커버리: 컨테이너 간 네트워킹과 로드밸런싱을 제공합니다.
- 스토리지 관리: 영구 볼륨 및 볼륨 클레임을 통해 데이터 저장을 관리합니다.
- 배치 실행: 일회성 작업 실행 및 스케줄링을 지원합니다.
##### 쿠버네티스 구성 요소
- 마스터 노드: 클러스터의 제어 및 관리 역할을 수행합니다.
- 워커 노드: 실제 애플리케이션을 실행하는 컴퓨팅 리소스입니다.
- 컨테이너 런타임: 컨테이너를 실행하는 소프트웨어 엔진입니다.
- kubelet: 워커 노드에서 실행되며 컨테이너의 생명주기를 관리합니다.
- kube-proxy: 네트워크 프록시 역할을 수행하여 서비스 간 네트워킹을 제공합니다.
#### 쿠버네티스의 장점
- 확장성: 클러스터 규모를 쉽게 확장할 수 있습니다.
- 자동화: 컨테이너 배포, 스케일링, 복구 등을 자동화할 수 있습니다.
- 고가용성: 복제본 관리와 자동 복구 기능으로 높은 가용성을 제공합니다.
- 오픈소스: 오픈소스 기반으로 개발되어 커뮤니티의 지속적인 발전이 가능합니다.

</div>
</details>

<details>
<summary>(꼬리질문) 그러면 컨테이너 오케스트레이션이란 무엇일까요?</summary>
<div>

컨테이너 오케스트레이션은 여러 컨테이너가 배포, 관리, 확장 및 네트워킹을 효율적으로 할 수 있도록 자동화하는 프로세스를 말합니다. 대규모 시스템에서 수백 또는 수천 개의 컨테이너를 관리하는 것은 매우 복잡할 수 있으므로, 컨테이너 오케스트레이션 도구는 이러한 과정을 단순화하고 자동화하여 개발자와 시스템 관리자의 부담을 줄여줍니다.

#### 컨테이너 오케스트레이션의 주요 기능
- 자동 배포: 애플리케이션을 컨테이너로 패키징하고, 이를 자동으로 배포합니다.
- 스케일링: 애플리케이션의 수요에 따라 컨테이너의 수를 자동으로 늘리거나 줄입니다.
- 로드 밸런싱과 서비스 발견: 외부 트래픽을 컨테이너로 분산시키고, 컨테이너 간 통신을 용이하게 합니다.
- health check & self-recovery: 컨테이너의 상태를 모니터링하고, 문제가 발생하면 자동으로 복구합니다.
- 컨테이너 간 통신과 네트워킹: 컨테이너가 서로 통신할 수 있도록 네트워크 설정을 관리합니다.
이러한 기능들은 컨테이너 오케스트레이션 도구를 사용하여 구현됩니다. Kubernetes, Docker Swarm, Apache Mesos는 가장 인기 있는 컨테이너 오케스트레이션 도구 중 일부입니다. 
</div>
</details>

<details>
<summary>도커와 쿠버네티스에 설명하고 두개가 어떻게 다른 것인지 설명해주세요</summary>
<div>

#### 도커(Docker)
- 도커는 애플리케이션을 컨테이너화하는 기술입니다. 이는 애플리케이션과 그 종속성을 컨테이너라는 격리된 환경에 패키징하여, 어떤 환경에서도 일관된 실행을 보장합니다.
- 도커는 이미지와 컨테이너를 관리하는데 사용되며, 도커 이미지는 애플리케이션을 실행하는데 필요한 모든 파일과 설정을 포함하고 있습니다. 컨테이너는 이 이미지를 실행한 인스턴스입니다.
- 도커 자체도 기본적인 컨테이너 오케스트레이션 기능을 제공하지만, 대규모 시스템에서는 보통 더 전문화된 오케스트레이션 도구를 사용합니다.
#### 쿠버네티스(Kubernetes)
- 쿠버네티스는 컨테이너화된 애플리케이션의 배포, 확장 및 관리를 자동화하기 위한 오픈소스 시스템입니다. 대규모 환경에서 컨테이너를 효율적으로 운영하기 위해 설계되었습니다.
- 쿠버네티스는 컨테이너의 배치, 스케일링, 업데이트, 서비스 디스커버리와 같은 복잡한 관리 작업을 자동화합니다. 이를 통해 개발자는 애플리케이션 개발에 더 집중할 수 있습니다.
- 쿠버네티스는 클러스터라는 형태로 구성되어 있으며, 여러 노드(컨테이너가 실행되는 물리적 또는 가상의 서버) 위에 애플리케이션을 분산시키고 관리합니다.
#### 도커와 쿠버네티스의 차이점
- 도커는 컨테이너를 생성하고 실행하는 기술의 핵심입니다. 컨테이너를 효율적으로 만들고, 실행하며, 관리하는 것에 초점을 맞춥니다.
- 쿠버네티스는 이러한 컨테이너들을 대규모로 관리하기 위한 시스템입니다. 여러 도커 컨테이너를 오케스트레이션하여, 애플리케이션의 배포, 확장 및 운영을 자동화합니다.

</div>
</details>

<details>
<summary>쿠버네티스 오토스케일링의 원리에 대해 설명해주세요</summary>
<div>

쿠버네티스(Kubernetes)의 오토스케일링(Auto-Scaling)은 시스템의 리소스 사용량에 따라 자동으로 파드(Pod)의 수를 조정하는 기능입니다.

##### Horizontal Pod Autoscaler (HPA)
HPA는 파드의 수를 수평적으로 조정하여 애플리케이션을 스케일링합니다. HPA는 주로 CPU 사용량이나 메모리 사용량과 같은 메트릭을 기반으로 파드의 수를 자동으로 늘리거나 줄입니다.

- 동작 원리: HPA는 쿠버네티스 메트릭 API를 통해 파드의 평균 리소스 사용량을 주기적으로 체크합니다. 사용량이 사용자가 설정한 임계값을 초과하면, HPA는 파드의 수를 늘려 부하를 분산시킵니다. 반대로, 리소스 사용량이 임계값 아래로 떨어지면 파드의 수를 줄여 리소스 낭비를 방지합니다.

##### Vertical Pod Autoscaler (VPA)
VPA는 파드의 리소스 요청(requests) 및 한도(limits)를 수직적으로 조정하여 애플리케이션을 스케일링합니다. VPA는 파드가 필요로 하는 CPU와 메모리 양을 자동으로 조정함으로써, 파드가 최적의 리소스를 사용하도록 합니다.

- 동작 원리: VPA는 파드의 리소스 사용 패턴을 분석하여, 각 파드에 대한 최적의 CPU와 메모리 요청 값을 추천합니다. 실제 리소스 사용량이 추천 값과 크게 다를 경우, VPA는 파드를 재시작하고 새로운 리소스 요청 값으로 파드를 구성합니다. 이 과정은 파드가 효율적으로 리소스를 사용하도록 돕습니다.

##### Cluster Autoscaler
또한, 쿠버네티스는 클러스터 자체의 노드 수를 자동으로 조정하는 Cluster Autoscaler도 지원합니다. 파드에 대한 요구가 증가하여 현재 클러스터의 자원으로는 충분하지 않을 때, Cluster Autoscaler는 자동으로 새로운 노드를 추가합니다. 반대로, 많은 노드들이 충분히 활용되지 않을 때는 노드를 제거하여 리소스를 효율적으로 관리합니다.

</div>
</details>

<details>
<summary>쿠버네티스 아키텍처에 대해 설명해주세요</summary>
<div>

마스터 노드(Master Node): 쿠버네티스 클러스터의 관리 및 조정을 담당합니다. 주요 구성 요소는 다음과 같습니다:

API 서버(api-server): 쿠버네티스 API를 제공하며, 사용자, 외부 시스템, 클러스터 내 다른 구성 요소와의 통신 인터페이스 역할을 합니다.
클러스터 저장소(etcd): 클러스터의 모든 상태와 구성을 저장하는 분산 키-값 저장소입니다.
스케줄러(scheduler): 새로 생성된 파드를 적절한 워커 노드에 할당하는 역할을 합니다.
컨트롤러 매니저(controller-manager): 노드 상태, 파드 복제, 엔드포인트 생성 등 다양한 클러스터 상태를 관리하고 조정합니다.
워커 노드(Worker Node): 실제 컨테이너화된 애플리케이션을 실행하는 물리적 또는 가상 머신입니다. 주요 구성 요소는 다음과 같습니다:

쿠블릿(kubelet): 마스터 노드의 명령을 받아 워커 노드에서 컨테이너를 실행하고 관리하는 에이전트입니다.
컨테이너 런타임(container runtime): 컨테이너 실행을 담당하는 소프트웨어로, Docker, containerd 등이 있습니다.
쿠브 프록시(kube-proxy): 워커 노드의 네트워크 프록시로, 쿠버네티스 서비스의 네트워크 통신을 관리합니다.
에티컬 etcd: 클러스터의 모든 구성 정보와 상태 정보를 저장하는 분산 키-값 저장소입니다. 클러스터의 브레인 역할을 하며, 모든 변경 사항은 여기에 저장됩니다.

서비스(Service): 파드의 동적인 집합에 안정적인 네트워크 주소를 제공합니다. 서비스는 파드 간의 통신 및 외부로의 노출을 용이하게 합니다.

네임스페이스(Namespace): 다수의 팀 또는 프로젝트 간의 리소스를 격리하여 관리할 수 있게 해주는 가상 클러스터입니다.


![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c83d9de0-c055-40be-92e6-a905a5c039c7/1c48c94f-c225-430c-a179-cb0a0dfc0393/Untitled.png)

클러스터의 VM은 크게 ContralPlane와 Worker노드로 나눠지며, 컨트롤 플레인과 노드는 각각의 VM을 사용합니다. 그리고 컨트롤 플레인 보다는 노드의 개수가 일반적으로 더 많습니다. 

마스터-노드 구조로, 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행합니다.

- 컨트롤 플레인은 노드들을 관리하는 역할을 하고, api-server(모든 요청을 처리하는 핵심 **모듈**), etcd(key-value 저장소), kube-scheduler(노드가 배정되지 않은 새로 생성된 파드를 감지하고 실행할 노드를 선택) 등으로 구성됩니다.
- 노드는 kubelet(파드의 생명 주기 관리)과 kube-proxy(파드로 연결되는 네트워크를 관리) 등으로 구성됩니다.

</div>
</details>

<details>
<summary>etcd 란 무엇인가?</summary>
<div>

https://tech.kakao.com/2021/12/20/kubernetes-etcd/

etcd는 분산 시스템에서 컨트롤 플레인에 위치하여 모든 데이터(구성 정보)와 서비스 발견을 위해 사용되는 오픈 소스 키-값 저장소입니다. 구글의 Chubby 논문에 영감을 받아 CoreOS 팀에 의해 개발되었습니다. 주로 분산 시스템의 데이터 일관성과 관리를 위해 사용되며, 특히 Kubernetes의 백엔드 스토리지로 널리 쓰입니다. etcd는 분산 시스템에 필요한 중요한 정보(구성 데이터, 상태 데이터, Kubernetes 메타데이터 등)를 저장하고 관리합니다.

#### etcd의 주요 특징

- 분산 시스템 지원: etcd는 Raft 합의 알고리즘을 사용하여 네트워크 파티션과 장애 상황에서도 데이터의 일관성을 유지합니다. 이를 통해 클러스터의 각 노드가 동일한 데이터를 보유하도록 보장합니다.

- 높은 가용성: etcd는 높은 가용성을 제공하기 위해 설계되었습니다. 클러스터 내의 여러 인스턴스를 통해 단일 실패 지점이 없도록 하며, 장애가 발생하더라도 시스템의 가용성을 유지할 수 있습니다.

- 키-값 저장소: etcd는 간단한 키-값 저장소로써 작동합니다. 이는 설정, 서비스 발견, 클러스터 상태 등의 데이터를 저장하고 관리하는 데에 사용됩니다.

- 보안: etcd는 TLS를 통한 통신 암호화와 클라이언트 인증을 지원하여 보안성을 강화합니다.

- 감시 및 이벤트 알림: etcd는 키의 변경 사항을 감시하고 변경 시 이벤트를 발생시키는 기능을 제공합니다. 이를 통해 분산 시스템 내에서 다양한 이벤트에 대응할 수 있습니다.

Kubernetes에서 etcd는 클러스터의 모든 상태 정보를 저장하는 중요한 컴포넌트로 사용됩니다. 예를 들어, 포드, 서비스, 레플리케이션 컨트롤러 등의 정보가 etcd에 저장되며, Kubernetes API 서버를 통해 이 정보에 접근합니다. 이를 통해 Kubernetes 클러스터의 상태를 일관되게 유지하고, 클러스터 관리를 효율적으로 수행할 수 있습니다.

---
##### Raft 합의 알고리즘
etcd는 분산 시스템에서 데이터의 일관성을 유지하기 위해 Raft 합의 알고리즘을 사용합니다. Raft는 분산된 시스템에서 모든 노드가 동일한 순서로 로그 항목을 적용하여 최종적으로 일관된 상태를 유지할 수 있도록 설계된 합의 알고리즘입니다. 이는 시스템의 안정성과 가용성을 보장하는 핵심 요소입니다.

##### Raft 합의 알고리즘의 주요 특징

- 리더 선출(Leader Election): Raft 클러스터는 리더와 팔로워로 구성되며, 모든 변경 사항은 리더를 통해 처리됩니다. 클러스터 내에서 리더가 실패하거나 네트워크 분할이 발생할 경우, 팔로워 중 하나가 새로운 리더로 선출됩니다.

- 로그 복제(Log Replication): 리더는 변경 사항(예: 키-값 쌍의 업데이트)을 로그 항목으로 클러스터의 다른 노드(팔로워)에 복제합니다. 팔로워들은 이 로그 항목을 자신의 로그에 추가하고, 리더에게 성공적으로 추가했음을 알립니다.

- 안전한 로그 적용(Safe Log Application): 로그 항목은 모든 노드가 동일한 순서로 적용해야 합니다. 이를 위해, 로그 항목은 대다수의 노드에 복제되고 합의된 후에만 적용됩니다. 이 과정은 시스템 전체에서 일관된 상태를 유지하는 데 필수적입니다.

- 임시 리더(Term Concept): Raft는 시간을 임시(terms)로 나누며, 각 임시는 새로운 리더 선출로 시작됩니다. 이는 클러스터의 리더십 변경을 관리하는 데 도움이 됩니다.

- 고가용성 및 장애 내성(Fault Tolerance): Raft 알고리즘은 네트워크 분할과 노드 실패를 포함한 다양한 장애 상황에서도 클러스터의 일관성과 가용성을 유지합니다. 대다수의 노드가 동작하는 한, 클러스터는 계속해서 정상적으로 작동할 수 있습니다.

Raft는 그 구현의 단순성과 이해하기 쉬운 합의 메커니즘으로 인해 많은 분산 시스템에서 선호되는 합의 알고리즘입니다. etcd는 이러한 Raft 알고리즘을 통해 분산 시스템에서의 데이터 일관성과 고가용성을 보장합니다.


</div>
</details>

<details>
<summary>
kubernetes 마스터를 구성할 때 홀수개로 하는 이유
</summary>
<div>

Kubernetes 클러스터에서 마스터 노드를 홀수 개로 구성하는 이유는 고가용성과 브레인 스플리팅(brain splitting) 문제를 방지하기 위함입니다. 이 구조는 특히 분산 시스템에서 결정을 내리는 데 있어 중요한 역할을 하는 컨센서스 알고리즘과 관련이 있습니다.

#### 고가용성(High Availability)
Kubernetes 클러스터는 여러 마스터 노드를 통해 고가용성을 달성할 수 있습니다. 만약 하나의 마스터 노드가 실패하더라도, 나머지 노드들이 클러스터의 관리와 운영을 계속 수행할 수 있습니다. 홀수 개의 마스터 노드를 사용하면, 과반수(majority)의 노드가 정상 작동해야만 전체 시스템의 결정을 내릴 수 있기 때문에 더 안정적인 시스템 운영이 가능합니다.

#### 브레인 스플리팅(Brain Splitting) 방지
브레인 스플리팅이란 네트워크 파티션으로 클러스터가 서로 소통할 수 없는 두 개 이상의 부분으로 나뉘었을 때 발생하는 문제입니다. 이 경우 각 부분은 다른 부분이 다운되었다고 판단하고 자신이 클러스터의 주된 부분이라고 생각할 수 있습니다. 홀수 개의 마스터 노드를 사용하면, 네트워크가 분리되더라도 하나의 부분이 반드시 과반수 이상의 노드를 포함하게 됩니다. 이를 통해 어느 한 쪽이 클러스터의 유효한 결정을 내리고 운영을 계속할 수 있게 합니다.

#### 컨센서스 알고리즘(Consensus Algorithm)
쿠버네티스는 etcd와 같은 분산 데이터 스토어를 사용하여 클러스터의 상태를 저장합니다. etcd는 Raft 컨센서스 알고리즘을 사용하는데, 이 알고리즘은 클러스터 내의 노드 간에 데이터를 일관되게 유지하기 위해 과반수의 동의가 필요합니다. 홀수 개의 노드를 가짐으로써, 클러스터가 결정을 내리고 쿠버네티스의 상태를 일관되게 유지하는 데 필요한 과반수를 더 쉽게 달성할 수 있습니다.

이러한 이유로, 쿠버네티스 클러스터의 마스터 노드는 고가용성과 안정성을 보장하기 위해 홀수 개로 구성하는 것이 일반적입니다.
</div>
</details>


<details>
<summary>CNI 란 무엇인가?</summary>
<div>

CNI(Container Network Interface, 컨테이너 네트워크 인터페이스)는 컨테이너의 네트워킹을 설정하기 위한 표준입니다. Kubernetes(쿠버네티스)와 같은 컨테이너 오케스트레이션 시스템에서 널리 사용됩니다. CNI는 컨테이너를 네트워크에 연결하고, 네트워크 인터페이스를 구성하며, IP 주소를 할당하는 등의 작업을 담당합니다.

CNI는 플러그인 아키텍처를 기반으로 설계되었습니다. 이는 다양한 네트워킹 솔루션들이 CNI 표준을 준수하는 플러그인 형태로 개발될 수 있음을 의미합니다. 따라서 사용자는 자신의 필요에 맞는 CNI 호환 네트워크 플러그인을 선택하여 사용할 수 있습니다. 예를 들어, Calico, Flannel, Weave Net 등 다양한 CNI 플러그인이 쿠버네티스 환경에서 네트워킹 솔루션으로 사용될 수 있습니다.

CNI의 주요 기능은 다음과 같습니다:

네트워크 인터페이스 생성 및 삭제: 컨테이너가 생성될 때 네트워크 인터페이스를 생성하고, 컨테이너가 제거될 때 해당 인터페이스를 삭제합니다.

IP 주소 할당: 컨테이너에 IP 주소를 할당합니다. 이는 고정 IP 또는 동적 IP가 될 수 있습니다.

라우팅 및 네트워크 정책 설정: 컨테이너 간의 통신을 위한 라우팅 규칙을 설정하고, 네트워크 정책을 적용하여 네트워크 보안을 강화합니다.

네트워크 리소스 관리: 네트워크 대역폭, QoS(Quality of Service) 등 네트워크 리소스의 관리를 지원할 수 있습니다.

</div>
</details>

<details>
<summary>ReplicaSet, Deployment에 대해 설명해주세요</summary>
<div>

ReplicaSet과 Deployment는 쿠버네티스(Kubernetes)에서 애플리케이션의 배포 및 스케일링을 관리하는 데 사용되는 리소스입니다. 이 두 리소스는 애플리케이션의 가용성을 유지하고 여러 인스턴스를 관리하는 데 중요한 역할을 합니다.

#### ReplicaSet
ReplicaSet은 지정된 수의 파드 복제본이 항상 실행되도록 보장합니다. ReplicaSet의 주요 목적은 가용성을 유지하는 것으로, 지정된 수의 파드 복제본이 항상 실행 중임을 보장합니다. 만약 파드가 실패하거나 삭제되면, ReplicaSet은 새로운 파드를 생성하여 원하는 상태를 유지합니다. ReplicaSet은 파드의 선택자(selector)를 통해 관리할 파드를 결정하며, 이 선택자는 레이블을 통해 파드를 식별합니다.

#### Deployment
Deployment는 ReplicaSet의 상위 개념으로, 애플리케이션의 배포, 업데이트 및 롤백을 관리합니다. Deployment는 ReplicaSet을 사용하여 배포의 상태를 관리하지만, 추가적으로 애플리케이션의 업데이트를 선언적으로 관리할 수 있는 기능을 제공합니다. 예를 들어, 새로운 애플리케이션 버전을 배포할 때 Deployment는 새로운 ReplicaSet을 생성하고, 점진적으로 기존 파드를 새로운 버전의 파드로 교체하여 업데이트를 진행합니다. 또한, 배포가 실패할 경우 이전 버전으로 롤백할 수 있는 기능도 제공합니다.

Deployment와 ReplicaSet의 주요 차이점은 Deployment가 애플리케이션의 업데이트와 롤백을 관리할 수 있다는 점입니다. ReplicaSet은 단순히 지정된 수의 파드 복제본이 실행되도록 유지하는 반면, Deployment는 애플리케이션의 배포 상태를 보다 세밀하게 제어할 수 있습니다.

쿠버네티스에서는 일반적으로 단순한 스케일링 이상의 기능이 필요할 때 Deployment를 사용하며, 이를 통해 애플리케이션의 배포, 업데이트 및 롤백을 쉽게 관리할 수 있습니다.

</div>
</details>

<details>
<summary>Job, CronJob에 대해 설명해주세요</summary>
<div>

### Job
Kubernetes에서 Job은 한 번 실행되고 종료되는 작업을 생성하기 위해 사용됩니다. Job은 하나 이상의 Pod를 생성하고, 지정된 수의 Pod가 성공적으로 완료될 때까지 실행됩니다. Pod가 실패할 경우, Job은 정책에 따라 실패한 Pod를 재시도할 수 있습니다.

##### Job의 주요 사용 사례
- 데이터 처리나 분석 작업
- 백업 작업
- 배치 처리 작업
###### Job의 예시
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
  template:
    spec:
      containers:
      - name: example
        image: busybox
        command: ["sh", "-c", "echo Hello Kubernetes! && sleep 30"]
      restartPolicy: Never
  backoffLimit: 4
```
위의 예시에서 backoffLimit은 Job이 실패했을 때 재시도하는 횟수를 제한합니다.

### CronJob
CronJob은 주기적으로 반복되는 Job을 실행하기 위해 사용됩니다. CronJob은 crontab(유닉스 계열 시스템에서 사용되는 시간 기반 작업 스케줄러)과 유사한 스케줄을 정의하여, 정해진 시간 또는 주기에 따라 Job을 자동으로 생성하고 실행합니다.

##### CronJob의 주요 사용 사례
- 정기적인 데이터베이스 백업
- 정기적인 이메일 발송
- 주기적인 리포트 생성
##### CronJob의 예시
```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: example-cronjob
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: example
            image: busybox
            command: ["sh", "-c", "date; echo Hello from the Kubernetes cluster"]
          restartPolicy: OnFailure
```
위의 예시에서 schedule 필드는 작업이 실행될 스케줄을 cron 형식으로 정의합니다. 이 경우, 매 5분마다 작업이 실행됩니다.

요약하자면, Job과 CronJob은 Kubernetes에서 일회성 작업과 주기적인 작업을 관리하는 데 유용한 리소스입니다. Job은 단일 작업을 실행하는 반면, CronJob은 주기적으로 반복되는 작업을 위해 사용됩니다.
</div>
</details>

<details>
<summary>PV, PVC 에 대해서 설명해주세요</summary>
<div>

### Persistent Volume (PV)
PV는 클러스터 내의 스토리지 볼륨을 나타내며, 관리자에 의해 프로비저닝되거나 스토리지 클래스를 사용하여 동적으로 프로비저닝될 수 있습니다. PV는 특정 스토리지 사이즈, 액세스 모드(예: ReadWriteOnce, ReadOnlyMany, ReadWriteMany), 그리고 실제 스토리지 시스템(예: NFS, iSCSI, 클라우드 제공 스토리지 서비스 등)에 대한 정보를 포함합니다. PV는 클러스터 리소스로, 여러 사용자 간에 공유될 수 있습니다.

### Persistent Volume Claim (PVC)
PVC는 사용자가 PV를 요청할 때 사용하는 리소스입니다. 사용자는 PVC를 통해 필요한 스토리지의 크기와 액세스 모드를 명시할 수 있으며, 쿠버네티스 시스템은 이 요구 사항을 만족하는 PV를 찾아 자동으로 바인딩합니다. PVC와 PV 간의 바인딩은 일대일 매핑으로 이루어지며, PVC가 PV에 바인딩되면, 그 PV는 다른 PVC에 의해 요청될 수 없습니다.

#### PV와 PVC의 작동 원리
- 스토리지 프로비저닝: 관리자는 PV를 수동으로 생성하거나, 스토리지 클래스를 통해 동적 프로비저닝을 설정할 수 있습니다.
- PVC 생성: 사용자는 필요한 스토리지의 크기와 액세스 모드를 지정하여 PVC를 생성합니다.
- PV 바인딩: 쿠버네티스는 PVC 요구사항을 만족하는 PV를 찾아 자동으로 바인딩합니다. 해당 PV는 이후 다른 PVC에 의해 요청될 수 없습니다. (1대1 양방향 바인딩 claimRef)
- 스토리지 사용: Pod는 PVC를 통해 바인딩된 PV에 접근하여 데이터를 읽고 쓸 수 있습니다.
- 정리: PVC를 삭제하면, 바인딩된 PV의 재사용 정책에 따라 PV가 자동으로 재사용(Retain)되거나 삭제될 수 있습니다.
PV와 PVC는 쿠버네티스에서 데이터의 영속성을 관리하고, 다양한 스토리지 옵션에 대한 추상화를 제공하여, 애플리케이션의 스토리지 요구사항을 유연하게 충족할 수 있게 합니다.

</div>
</details>

<details>
<summary>ServiceAccount 에 대해서 설명해주세요</summary>
<div>

쿠버네티스(Kubernetes)에서 ServiceAccount(서비스 계정)은 파드(Pod)가 쿠버네티스 API와 상호 작용할 때 사용하는 계정입니다. 각 서비스 계정은 특정 네임스페이스(Namespace)에 속하며, 주로 파드 내에서 실행되는 애플리케이션이 쿠버네티스 API와 안전하게 통신할 수 있도록 인증 및 권한 부여를 제공합니다.

##### 서비스 계정의 주요 특징 및 용도
- 자동 생성과 관리: 쿠버네티스는 기본적으로 'default' 서비스 계정을 자동으로 생성하고, 파드가 명시적으로 서비스 계정을 지정하지 않는 경우 이 계정을 사용합니다. 사용자는 특정 작업이나 애플리케이션의 권한 요구사항에 맞춰 추가적인 서비스 계정을 생성할 수 있습니다.

- API 접근 제어: 서비스 계정은 파드가 쿠버네티스 API에 접근할 때 필요한 인증 정보를 포함합니다. 이를 통해 파드는 쿠버네티스 클러스터 내의 다른 리소스를 조회, 생성, 수정하는 등의 작업을 수행할 수 있습니다.

- Role-Based Access Control(RBAC)과의 연동: 서비스 계정은 RBAC 정책과 연동되어, 파드 또는 애플리케이션이 수행할 수 있는 작업의 범위를 제한합니다. 이를 통해 최소 권한 원칙을 적용하고, 시스템 보안을 강화할 수 있습니다.

- Secrets 관리: 서비스 계정은 Secret 리소스를 사용하여 API 토큰과 같은 인증 정보를 안전하게 저장하고 관리합니다. 이러한 Secret은 자동으로 파드에 마운트되어 애플리케이션이 API와 통신할 때 사용될 수 있습니다.

##### 서비스 계정 사용 예
- 외부 데이터베이스 접근: 파드 내부에서 실행되는 애플리케이션이 외부 데이터베이스에 접근할 필요가 있을 때, 서비스 계정을 통해 필요한 인증 정보를 안전하게 관리하고 사용할 수 있습니다.

- CI/CD 파이프라인: 쿠버네티스 클러스터 내에서 실행되는 CI/CD 작업에서 서비스 계정을 사용하여 리소스를 생성, 수정, 삭제하는 등의 작업을 안전하게 수행할 수 있습니다.

</div>
</details>

<details>
<summary>
rbac는 무엇이고 어떻게 구성되는지
</summary>
<div>

RBAC(Role-Based Access Control)는 쿠버네티스(Kubernetes)에서 리소스에 대한 접근 제어를 관리하는 데 사용되는 중요한 메커니즘입니다. 사용자, 서비스 계정 또는 파드가 쿠버네티스 API에 요청을 할 때, RBAC는 해당 요청이 허용되는지 여부를 결정합니다. RBAC는 역할(Role)과 역할 바인딩(RoleBinding) 또는 클러스터 역할(ClusterRole)과 클러스터 역할 바인딩(ClusterRoleBinding)을 사용하여 구성됩니다.

#### RBAC 구성 요소
##### Role/ClusterRole
- Role은 특정 네임스페이스에 국한된 권한을 정의합니다. 예를 들어, 특정 네임스페이스에서 파드를 조회하거나 생성할 수 있는 권한을 부여할 수 있습니다.
- ClusterRole은 클러스터 수준에서 권한을 정의하며, 이는 클러스터 전체 리소스(예: 노드) 또는 네임스페이스 자체와 같이 네임스페이스에 속하지 않는 리소스에 대한 접근을 제어할 수 있습니다.
##### RoleBinding/ClusterRoleBinding:
- RoleBinding은 특정 네임스페이스 내에서 하나 이상의 사용자, 그룹 또는 서비스 계정에게 Role을 할당합니다.
- ClusterRoleBinding은 클러스터 전체 수준에서 하나 이상의 사용자, 그룹 또는 서비스 계정에게 ClusterRole을 할당합니다.

#### RBAC 설정 예시
##### Role 생성 예시

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: mynamespace
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```
##### RoleBinding 생성 예시
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: mynamespace
subjects:
- kind: User
  name: "janedoe"
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
#### 주요 고려 사항
- RBAC는 최소 권한 원칙을 따라야 합니다. 사용자 또는 서비스 계정에 필요한 최소한의 권한만 부여해야 합니다.
- 네임스페이스 수준의 리소스 관리가 필요한 경우 Role/RoleBinding을, 클러스터 전체 리소스 관리가 필요한 경우 ClusterRole/ClusterRoleBinding을 사용합니다.
- RBAC 설정은 보안과 관련된 중요한 부분이므로, 권한 할당 시 신중을 기해야 합니다.
- RBAC를 통해 쿠버네티스 클러스터 내에서의 세밀한 접근 제어가 가능해지며, 이를 통해 보안을 강화하고 필요한 사용자나 서비스 계정만이 특정 리소스에 접근할 수 있도록 할 수 있습니다.
</div>
</details>

<details>
<summary>쿠버네티스 context에 대해 설명해주세요</summary>
<div>

쿠버네티스에서 context는 사용자가 하나 이상의 쿠버네티스 클러스터에 접근하기 위해 사용하는 구성 정보의 집합(쿠버네티스 클러스터, 사용자, 네임스페이스 정보를 조합)입니다. kubectl 명령어를 사용할 때, context는 어떤 쿠버네티스 클러스터에 연결할지, 어떤 사용자의 인증 정보를 사용할지, 그리고 어떤 네임스페이스에서 작업할지를 결정합니다.

#### 주요 구성 요소
- 클러스터: 클러스터의 서버 주소와 인증에 필요한 정보(인증서, 키 등)를 포함합니다.
- 사용자: API 서버에 접근할 때 사용할 인증 정보(토큰, 클라이언트 인증서 등)를 포함합니다.
- 네임스페이스: 기본적으로 사용할 네임스페이스를 지정합니다. 네임스페이스를 지정하지 않으면 default 네임스페이스가 사용됩니다.
#### Context 설정하기
쿠버네티스 context는 ~/.kube/config 파일에 저장됩니다. 이 파일은 클러스터, 사용자, context를 정의하고, 현재 사용 중인 context를 지정합니다. 사용자는 kubectl config 명령어를 사용하여 이 파일을 관리할 수 있습니다. 예를 들어, kubectl config use-context my-context 명령어를 사용하여 my-context라는 context를 활성화할 수 있습니다.(`kubectl config get-contexts` 목록 조회)

#### 사용 사례
- 다중 클러스터 관리: 개발, 스테이징, 프로덕션 등 다양한 환경의 클러스터를 관리해야 하는 경우, 각 환경에 대한 context를 설정하고 필요에 따라 쉽게 전환할 수 있습니다.
- 접근 권한 분리: 다른 사용자 또는 네임스페이스에 대해 다른 접근 권한을 가지는 경우, 각 사용 사례에 맞는 context를 설정하여 보안을 강화할 수 있습니다.


kubectl config 명령어를 통해 현재 설정된 context를 확인하고, 새로운 context를 추가하거나 기존의 것을 수정할 수 있습니다. 이를 통해 사용자는 쿠버네티스 클러스터와의 상호작용을 보다 효율적으로 관리할 수 있습니다.
</div>
</details>

<details>
<summary>쿠버네티스가 필요한 이유가 무엇일까요?</summary>
<div>

#### 확장성 및 유연성
- 애플리케이션의 워크로드에 따라 신속하게 확장하거나 축소할 수 있습니다.
- 다양한 서비스로 구성된 애플리케이션을 효과적으로 관리할 수 있습니다.

#### 컨테이너 관리 효율화
- 프로덕션 환경에서 컨테이너를 관리하는 효율적인 방법을 제공합니다.
- 애플리케이션을 쉽게 패키징하고 실행할 수 있습니다.

#### 자동화된 배포 및 관리
- 컨테이너화된 애플리케이션의 배포, 관리, 확장을 자동화합니다.
- 로드 밸런싱, 스토리지 관리, 네트워킹 등의 기능을 제공합니다.

#### 높은 가용성 및 신뢰성
- 애플리케이션의 가용성과 신뢰성을 높일 수 있습니다.
- 자동 복구, 자동 확장 등의 기능으로 안정적인 운영이 가능합니다. 

#### 생산성 향상
- 개발자와 운영팀의 생산성을 높일 수 있습니다. 
- 애플리케이션의 이식성과 포터빌리티를 높여줍니다. 
</div>
</details>

<details>
<summary>스케일 아웃(Scale-out)과 스케일 업(Scale-up)의 차이를 설명하라.</summary>
<div>

### 스케일 아웃(Scale-out)
스케일 아웃은 시스템의 성능을 향상시키기 위해 추가적인 노드나 인스턴스를 시스템에 추가하는 방식입니다. 이는 서버의 수를 늘림으로써 처리 능력을 확장하는 방식으로, 분산 시스템에서 흔히 사용됩니다. 스케일 아웃 방식은 일반적으로 탄력성이 뛰어나고, 필요에 따라 리소스를 유연하게 추가하거나 제거할 수 있는 장점이 있습니다. 예를 들어, 웹 애플리케이션의 트래픽이 증가할 때, 추가적인 웹 서버를 배치하여 부하를 분산시키는 것이 스케일 아웃의 예입니다.

### 스케일 업(Scale-up)
스케일 업은 기존의 시스템이나 서버의 성능을 강화하는 방법입니다. 이는 하드웨어의 성능을 향상시키기 위해 CPU, RAM, 스토리지 같은 리소스를 추가하는 방식입니다. 스케일 업은 시스템의 복잡성을 크게 증가시키지 않으면서 성능을 향상시킬 수 있는 방법으로, 간단한 작업이나 단일 시스템의 성능을 빠르게 향상시키고 싶을 때 유용합니다. 예를 들어, 데이터베이스 서버에 더 많은 메모리를 추가하거나 더 빠른 CPU로 업그레이드하는 것이 스케일 업의 예입니다.

##### 차이점
스케일 아웃은 추가적인 하드웨어나 서버를 도입하여 시스템을 확장하는 반면, 스케일 업은 기존의 시스템 또는 서버 내부의 리소스를 업그레이드하여 성능을 향상시킵니다.
스케일 아웃은 일반적으로 탄력성과 확장성이 뛰어나고, 클라우드 환경에서 효과적인 접근 방식입니다. 반면, 스케일 업은 하드웨어 한계에 도달할 때까지만 성능을 향상시킬 수 있으며, 일정 수준 이상의 성능 향상을 위해서는 비용이 기하급수적으로 증가할 수 있습니다.
스케일 아웃은 시스템의 고가용성과 장애 복구 능력을 향상시킬 수 있지만, 스케일 업은 단일 장애 지점(Single Point of Failure, SPOF) 문제를 해결하지 못할 수 있습니다.

</div>
</details>


<details>
<summary>
kubernetes의 버전 업그레이드 과정
</summary>

<div>
##### 공식 문서 가이드가 잘나와있기 때문에 참고해서 준비!
https://kubernetes.io/ko/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

Kubernetes 클러스터를 새로운 버전으로 업그레이드하는 것은 중요한 작업입니다. 새로운 기능과 버그 수정을 활용하고 보안 취약점을 해결하기 위해서입니다.

##### Kubernetes 클러스터 업그레이드 시 주요 고려사항
- 업그레이드 경로: 현재 버전에서 목표 버전으로 안전하게 업그레이드할 수 있는 경로를 선택해야 합니다.
- 데이터 백업: 업그레이드 전 클러스터의 중요한 데이터를 백업해야 합니다.
- 워크로드 마이그레이션: 업그레이드 중 워크로드가 중단되지 않도록 관리해야 합니다.
- 호환성 검토: 새로운 버전의 Kubernetes가 기존 애플리케이션과 호환되는지 확인해야 합니다.

kubeadm을 사용하여 Kubernetes 클러스터를 업그레이드하는 과정은 몇 가지 주요 단계로 나눌 수 있습니다. 여기에는 사전 준비 작업, 컨트롤 플레인 업그레이드, 그리고 워커 노드 업그레이드가 포함됩니다. 아래의 지침은 고가용성(High Availability, HA) 설정에 있는 클러스터에도 적용될 수 있으나, HA 클러스터의 경우 추가 고려 사항이 필요할 수 있습니다.

1. 사전 준비
   - 업그레이드 전 확인 사항
     - 현재 실행 중인 클러스터의 버전을 확인합니다.
     - 대상 버전과의 호환성, 릴리스 노트, 변경 사항을 검토합니다.
   - 백업
     - 중요한 데이터와 설정, 특히 etcd 데이터베이스의 백업을 수행합니다.
   - 도구 업그레이드
     - kubeadm, kubectl, kubelet의 버전을 클러스터 업그레이드 대상 버전으로 업그레이드합니다.
2. 컨트롤 플레인 업그레이드
   - 컨트롤 플레인 노드 준비
     - 업그레이드를 시작하기 전에 컨트롤 플레인 노드를 고려합니다. HA 클러스터의 경우 하나의 컨트롤 플레인 노드씩 순차적으로 업그레이드합니다.(이때 첫번째 컨트롤 플레인 노드만 `kubeadm upgrade plan`과 cni 플러그인 업데이트를 수행하면 되고 두번째 컨트롤 플레인 노드는 `kubeadm upgrade apply` 대신 `kubeadm upgrade node` 명령어를 사용한다.)
   - kubeadm 업그레이드 실행
     - `kubeadm upgrade apply <version>` 명령을 사용하여 컨트롤 플레인을 업그레이드합니다. 여기서 <version>은 목표 버전입니다.
   - kubelet과 kubectl 업그레이드
     - `kubectl drain <node-to-drain> --ignore-daemonsets`
     - 컨트롤 플레인 노드에서 kubelet을 업그레이드하고 재시작합니다. (kubectl도 업그레이드합니다.)
     - `apt-mark unhold kubelet kubectl && apt-get update && apt-get install -y kubelet=1.30.x-00 kubectl=1.30.x-00 && apt-mark hold kubelet kubectl`
     - `sudo systemctl daemon-reload && sudo systemctl restart kubelet`
     - `kubectl uncordon <node-to-uncordon>`

3. 워커 노드 업그레이드
   - 노드 드레이닝
     - 업그레이드할 워커 노드에서 kubectl drain <node-name> 명령을 사용하여 안전하게 파드를 이동시킵니다.
   - kubeadm 업그레이드
     - kubeadm upgrade node 명령을 사용하여 각 워커 노드의 kubeadm 인스턴스를 업그레이드합니다.
   - kubelet과 kubectl 업그레이드
     - 각 워커 노드에서 kubelet을 업그레이드하고 재시작합니다.
     - 필요한 경우 kubectl을 업그레이드합니다.
   - 노드 언코딩
   - 업그레이드가 완료된 후, kubectl uncordon <node-name> 명령을 사용하여 노드에 파드 스케줄링을 다시 허용합니다.
4. 검증 및 정리
- 모든 노드가 올바르게 업그레이드되었는지 확인합니다.
- 클러스터가 정상적으로 작동하는지 테스트합니다.
- 필요한 경우, 백업에서 복원할 준비를 합니다.


</div>
</details>


<details>
<summary>
MSA vs 모놀리식 구조의 차이
</summary>
<div>

#### 아키텍처 구조
- 모놀리식 구조: 단일 큰 애플리케이션으로 구성됩니다. 모든 기능이 하나의 코드베이스에 통합되어 있습니다.
- MSA: 큰 애플리케이션을 독립적으로 실행 가능한 작은 서비스들로 분리합니다. 각 서비스는 자체 코드베이스와 데이터베이스를 가집니다.

#### 개발 및 배포
- 모놀리식 구조: 전체 애플리케이션을 한 번에 개발하고 배포합니다.
- MSA: 각 서비스를 독립적으로 개발하고 배포할 수 있습니다.

#### 확장성
- 모놀리식 구조: 전체 애플리케이션을 확장해야 하므로 확장성이 제한적입니다.
- MSA: 각 서비스를 독립적으로 확장할 수 있어 확장성이 높습니다.

#### 복잡도
- 모놀리식 구조: 전체 애플리케이션을 이해하고 관리해야 하므로 복잡도가 높습니다.
- MSA: 각 서비스가 독립적이므로 복잡도가 상대적으로 낮습니다.

#### 장애 격리
- 모놀리식 구조: 한 부분의 장애가 전체 애플리케이션에 영향을 줄 수 있습니다.
- MSA: 각 서비스의 장애가 다른 서비스에 영향을 주지 않습니다.

##### 추가 정보
모놀리식 구조는 작은 프로젝트에 적합하지만, 규모가 커질수록 관리가 어려워집니다.
MSA는 대규모 프로젝트에 적합하며, 서비스 간 독립성으로 인해 배포, 확장, 유지보수가 용이합니다.
요약하면, MSA는 모놀리식 구조에 비해 아키텍처 구조, 개발 및 배포, 확장성, 복잡도, 장애 격리 등 다양한 측면에서 장점을 가지고 있습니다. 프로젝트의 규모와 요구사항에 따라 적절한 아키텍처를 선택하는 것이 중요합니다.
</div>
</details>

<details>
<summary>
Client에서 Kubernetes의 Cluster 로 접근하는 방법
</summary>
<div>
**`kubectl` 외에도 Kubernetes API에 직접 접근하는 방법, Kubernetes 대시보드 사용, 그리고 클라우드 제공업체가 제공하는 관리 콘솔 등을 통해서도 클러스터에 접근할 수 있습니다.**

또한 언어별 라이브러리를 이용하는 방법도 있습니다.
</div>
</details>

<details>
<summary>
calico overlay network란?
</summary>
<div>

Calico는 컨테이너, 가상 머신, 네이티브 호스트 워크로드를 위한 네트워크 및 네트워크 보안 솔루션을 제공하는 오픈소스 프로젝트입니다. Kubernetes 클러스터에서 네트워크 플러그인으로 널리 사용되며, 높은 성능과 간단한 네트워킹 모델, 네트워크 폴리시를 통한 상세한 네트워크 액세스 제어 기능을 제공합니다.

Calico는 오버레이 네트워크를 선택적으로 사용할 수 있는데, 오버레이 네트워크는 물리적 네트워크 위에 가상 네트워크를 구축하는 기술입니다. 이를 통해 컨테이너들이 동일한 물리 네트워크에 있지 않아도 서로 통신할 수 있게 해줍니다. Calico에서는 주로 다음 두 가지 오버레이 네트워킹 모드를 제공합니다

IP-in-IP (IP encapsulation): IP-in-IP 모드는 한 IP 패킷을 다른 IP 패킷 내에 캡슐화하여, 중간 네트워크가 원본 패킷의 내용을 몰라도 목적지에 도달할 수 있도록 합니다. 이 방식은 주로 L3 네트워킹을 사용하는 환경에서, 다른 클러스터 노드로의 트래픽을 라우팅할 때 사용됩니다.VXLAN (Virtual Extensible LAN): VXLAN은 네트워크 내의 가상 네트워크를 생성하기 위해 사용되는 오버레이 네트워킹 기술입니다. VXLAN은 매우 큰 수의 격리된 네트워크 세그먼트를 생성할 수 있으며, IP-in-IP에 비해 더 복잡한 네트워크 환경에서 활용될 수 있습니다.

calico를 사용할 때, 오버레이 네트워킹은 선택적입니다. 즉, 클러스터가 동일한 L2 네트워크 내에 있고, 네트워크 격리가 물리적 또는 다른 방법으로 이미 충분히 제공되는 경우 오버레이 네트워크 없이 네이티브 L3 라우팅을 사용할 수 있습니다. 이는 오버레이 네트워크를 사용하지 않을 때 성능상의 이점을 제공할 수 있습니다. 반면, 다양한 클라우드 제공 업체 또는 온프레미스 환경에서 워크로드를 운영할 경우 오버레이 네트워킹이 네트워크 격리 및 통신을 위해 필요할 수 있습니다.**

Calico의 오버레이 네트워킹 기능은 클러스터의 네트워크 요구 사항에 따라 유연하게 구성할 수 있어, 다양한 환경과 요구 사항에 맞는 네트워킹 솔루션을 제공합니다.

</div>
</details>


<details>
<summary>
kubernetes taint & tolerations
</summary>
<div>

**쿠버네티스(Kubernetes)에서 테인트(Taint)란 특정 노드(Node)에 파드(Pod)가 스케줄되는 것을 허용하거나 금지하기 위해 사용되는 메커니즘입니다. 테인트는 노드에 부여되며, 파드가 해당 노드에 스케줄되기 위해서는 해당 테인트를 "용인"할 수 있는 적절한 톨러레이션(Toleration)을 가져야 합니다.**

**테인트는 주로 세 가지 속성으로 구성됩니다: 키(key), 값(value), 그리고 효과(effect). 테인트의 효과에는 주로 다음 세 가지가 있습니다:**

**NoSchedule: 이 효과를 가진 테인트가 노드에 있으면, 적절한 톨러레이션이 없는 파드는 그 노드에 스케줄될 수 없습니다.PreferNoSchedule: 이는 NoSchedule보다 더 유연한 옵션으로, 쿠버네티스 스케줄러가 가능한 해당 노드를 피하려고 하지만, 필요한 경우에는 파드를 스케줄할 수 있습니다.NoExecute: 이 효과를 가진 테인트가 노드에 추가되면, 적절한 톨러레이션이 없는 파드는 노드에서 즉시 제거되고, 새로운 파드는 스케줄될 수 없습니다.**

**테인트를 노드에 추가하기 위해서는 `kubectl taint` 명령어를 사용할 수 있습니다. 예를 들어, 노드에 "NoSchedule" 효과를 가진 테인트를 추가하려면 다음과 같이 명령을 실행할 수 있습니다:**

```bash
bashkubectl taint nodes <노드 이름> key=value:NoSchedule

```

**테인트와 반대로, 톨러레이션은 파드 사양에 정의되며, 특정 테인트를 "용인"할 수 있게 해 줍니다. 즉, 테인트가 있는 노드에도 파드가 스케줄될 수 있게 해 줍니다. 톨러레이션은 파드의 `tolerations` 필드에 추가됩니다.**

**테인트와 톨러레이션을 사용함으로써, 쿠버네티스 클러스터에서 파드가 스케줄링되는 방식을 더 세밀하게 제어할 수 있습니다. 이는 특정 작업을 수행할 수 있는 노드에만 파드를 스케줄하는 등의 경우에 유용하게 사용될 수 있습니다.**
</div>
</details>

<details>
<summary>
kubernetes service란?
</summary>
<div>

Kubernetes에서 서비스(Service)는 애플리케이션의 컴포넌트들 사이, 혹은 외부와의 통신을 중계하는 추상적인 개념입니다. 서비스는 쿠버네티스 클러스터 내에서 실행되는 포드(Pod)들에 대한 지속적인 접근 방법을 제공합니다. 포드는 일반적으로 동적으로 생성되고 소멸되기 때문에, IP 주소가 자주 바뀔 수 있습니다. 서비스는 이러한 포드들의 논리적 집합에 대해 안정적인 주소와 액세스 방법을 제공함으로써, 이 문제를 해결합니다.

### 서비스의 주요 기능
- 로드 밸런싱: 서비스는 클라이언트 요청을 서비스에 연결된 여러 포드 사이에 분산시켜 로드 밸런싱을 수행합니다. 이는 애플리케이션의 가용성과 확장성을 향상시킵니다.
- 서비스 디스커버리: 서비스는 쿠버네티스 내부 DNS에 등록되어, 다른 컴포넌트들이 서비스 이름을 통해 상호 연결될 수 있도록 합니다. 이를 통해 포드들이 동적으로 변해도, 서비스에 연결된 컴포넌트들은 안정적으로 통신할 수 있습니다.
- 안정적인 통신: 서비스는 포드 그룹에 대한 안정적인 접근점을 제공합니다. 이는 포드의 IP 주소가 변경되더라도, 서비스를 통해 지속적으로 통신이 가능하게 합니다.

### 서비스 타입
- ClusterIP: 기본 서비스 타입으로, 클러스터 내부에서만 접근 가능한 내부 IP를 할당받습니다.
- NodePort: 클러스터 외부에서 접근 가능하도록, 모든 노드의 특정 포트를 통해 서비스에 접근할 수 있게 합니다.
- LoadBalancer: 클라우드 제공자의 로드 밸런서를 사용하여 서비스에 외부 IP 주소를 할당하고, 외부에서 서비스에 접근할 수 있게 합니다.
- ExternalName: 서비스를 쿠버네티스 클러스터 외부의 URL로 매핑합니다. 이 타입은 포드와 직접적으로 연결되지 않습니다.
</div>
</details>


<details>
<summary>
kubernetes ingress란
</summary>
<div>

Kubernetes Ingress는 클러스터 내부의 서비스에 외부에서 접근할 수 있도록 하는 API 객체입니다. Ingress를 사용하면 HTTP, HTTPS와 같은 트래픽을 클러스터 내의 서비스로 라우팅할 수 있으며, 로드 밸런싱, SSL/TLS 종료 및 이름 기반의 가상 호스팅을 제공합니다.

### Ingress의 주요 기능
- 경로 기반 라우팅: 클라이언트 요청의 URL 경로를 기반으로 다른 서비스로 트래픽을 라우팅합니다. 예를 들어, /video 요청을 비디오 관련 서비스로, /image 요청을 이미지 관련 서비스로 라우팅할 수 있습니다.
- 호스트 기반 라우팅: 클라이언트 요청의 도메인 이름을 기반으로 트래픽을 라우팅합니다. 이를 통해 단일 IP 주소를 사용하여 여러 도메인을 처리할 수 있습니다.
- SSL/TLS 종료: Ingress 컨트롤러는 SSL/TLS 종료를 처리할 수 있으며, 이를 통해 클러스터 내부의 통신을 보호할 수 있습니다.

Ingress를 사용하기 위해서는 Ingress 컨트롤러가 필요합니다. Ingress 컨트롤러는 Ingress 규칙에 따라 트래픽을 적절한 서비스로 라우팅하는 역할을 합니다. Kubernetes는 여러 Ingress 컨트롤러를 지원하며, 사용자는 자신의 요구 사항에 맞는 Ingress 컨트롤러를 선택하여 사용할 수 있습니다.

### Ingress 사용 사례
- 단일 IP 주소를 사용하여 여러 도메인을 호스팅하는 경우
- HTTPS를 통한 안전한 통신이 필요한 경우
- 경로 또는 호스트 기반으로 트래픽을 라우팅해야 하는 경우
</div>
</details>


<details>
<summary>
모니터링 툴을 사용해본 적이 있는가? 있다면 그에 관해 설명하라.
</summary>
<div>

kubernetes 자체에서 지원되는 kubernetes dashboard가 있고 prometheus와 grafana 조합으로 kubernetes 내에서 실행 중인 애플리케이션과 인프라의 실시간 성능 모니터링 및 분석을 할 수 있습니다.

### Prometheus
Prometheus는 오픈 소스 모니터링 및 알람 시스템으로, 구글의 Borgmon에서 영감을 받아 개발되었습니다. 주로 시계열 데이터를 수집하고 저장하는 데 사용되며, Kubernetes와 같은 동적인 서비스 디스커버리 환경에서 효율적으로 작동합니다.
#### Prometheus 기능
- 다양한 데이터 소스 수집: Prometheus는 HTTP 경로를 통해 메트릭을 노출하는 어떤 서비스로부터도 데이터를 수집할 수 있습니다. 이는 Kubernetes 상의 서비스들이 Prometheus와 쉽게 통합될 수 있음을 의미합니다.
- 서비스 디스커버리: Kubernetes 클러스터 내에서 실행 중인 서비스를 자동으로 발견하고 모니터링할 수 있습니다.
- 강력한 쿼리 언어: Prometheus는 자체 쿼리 언어인 PromQL을 제공하여, 수집된 데이터를 기반으로 복잡한 질의와 계산을 수행할 수 있습니다.
- 알람: Prometheus는 Alertmanager와 통합되어, 지정된 규칙에 따라 알람를 생성하고 관리할 수 있습니다.
### Grafana
Grafana는 다양한 데이터 소스로부터 수집된 데이터를 시각화하는 데 사용되는 오픈 소스 분석 및 모니터링 소프트웨어입니다. Grafana는 사용자가 대시보드를 통해 데이터를 쉽게 이해하고 분석할 수 있도록 도와줍니다. 
#### Grafana 기능
- 다양한 데이터 시각화 옵션: Grafana는 그래프, 테이블, 히트맵 등 다양한 시각화 옵션을 제공합니다. 사용자는 이를 통해 데이터를 쉽게 분석하고 이해할 수 있습니다.
- 강력한 대시보드: Grafana 대시보드는 사용자가 중요한 메트릭을 한눈에 볼 수 있도록 커스터마이징할 수 있습니다.
- 알람: Grafana는 데이터 트렌드를 모니터링하고 특정 조건이 충족될 때 알람을 발송하는 기능을 제공합니다.
### Kubernetes에서의 통합
- Prometheus 설치: Prometheus 서버를 Kubernetes 클러스터에 설치하고, 서비스 디스커버리를 활성화하여 클러스터 내의 서비스를 자동으로 발견하도록 설정합니다.
- 메트릭 수집: Kubernetes 클러스터 및 클러스터 내에서 실행 중인 애플리케이션으로부터 메트릭을 수집합니다. (이때 PodMonitor 종류의 리소스를 사용)
- Grafana 설치: Grafana를 Kubernetes 클러스터에 설치합니다.
- Prometheus를 데이터 소스로 Grafana 구성: Grafana에서 Prometheus를 데이터 소스로 추가하여, 수집된 메트릭을 시각화합니다.


### Thanos
Thanos는 Prometheus의 확장 프로젝트로, 크게 확장성과 장기 저장 문제를 해결하기 위해 설계되었습니다. Prometheus 인스턴스가 증가함에 따라 데이터를 효율적으로 관리하고, 데이터 손실 위험 없이 장기간 저장할 수 있는 방법을 제공합니다. 
#### Thanos 특징
- 장기 데이터 보관: Thanos는 저렴한 객체 스토리지(S3, GCS 등)를 사용하여 Prometheus 메트릭 데이터를 장기간 보관할 수 있습니다.
- 확장성: Thanos는 여러 Prometheus 인스턴스에서 수집된 데이터를 쿼리할 수 있는 글로벌 쿼리 레이어를 제공합니다. 이를 통해 대규모 시스템에서도 효율적인 데이터 질의가 가능합니다.
- 고가용성: Thanos는 데이터 복제를 통해 메트릭 데이터의 고가용성을 보장합니다. 이는 데이터 손실 위험을 줄이고, 시스템의 신뢰성을 높입니다.
- 효율적인 데이터 압축 및 다운샘플링: 장기 저장을 위한 데이터의 압축과 다운샘플링을 지원하여, 스토리지 비용을 최적화합니다.
Thanos는 Prometheus의 기능을 확장하여 대규모 시스템에서의 모니터링 요구 사항을 충족시키며, 장기 데이터 보관 및 분석을 가능하게 합니다. 이러한 이유로 Prometheus와 Thanos는 함께 사용될 때 클라우드 네이티브 환경에서의 모니터링 및 알림 시스템을 위한 강력한 조합을 이룹니다.
</div>
</details>


<details>
<summary>
kubernetes에서 계정 권한 설정하는 방법
</summary>
<div>

Kubernetes에서 계정 권한을 설정하는 것은 주로 Role-Based Access Control (RBAC)을 통해 이루어집니다. RBAC는 사용자, 그룹, 또는 서비스 계정에 대한 권한을 제어하는 데 사용되는 방법입니다. Kubernetes 클러스터 내의 리소스에 대한 접근을 세밀하게 제어할 수 있게 해주며, 네임스페이스 수준에서 또는 클러스터 전체 수준에서 권한을 설정할 수 있습니다.

#### 기본 개념
Role과 ClusterRole: 이들은 권한의 집합을 정의합니다. Role은 특정 네임스페이스 내에서만 유효한 권한을, ClusterRole은 클러스터 전체에서 유효한 권한을 정의합니다.
RoleBinding과 ClusterRoleBinding: 이들은 특정 사용자, 그룹, 또는 서비스 계정에게 Role이나 ClusterRole을 할당합니다. RoleBinding은 특정 네임스페이스에, ClusterRoleBinding은 클러스터 전체에 적용됩니다.
#### 권한 설정 예제
1. Role 생성하기: 특정 네임스페이스에서 Pod 리소스를 조회할 수 있는 권한을 정의하는 Role을 생성합니다.
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: your-namespace
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```
2. RoleBinding 생성하기: 특정 사용자에게 위에서 생성한 Role을 할당합니다.
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: your-namespace
subjects:
- kind: User
  name: "your-user-name"
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
3. ClusterRole 생성하기: 클러스터 전체에서 모든 네임스페이스의 Pod 리소스를 조회할 수 있는 권한을 정의하는 ClusterRole을 생성합니다.
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-reader-global
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```
4. ClusterRoleBinding 생성하기: 특정 사용자에게 위에서 생성한 ClusterRole을 할당합니다.
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-pods-global
subjects:
- kind: User
  name: "your-user-name"
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: pod-reader-global
  apiGroup: rbac.authorization.k8s.io
```

</div>
</details>

